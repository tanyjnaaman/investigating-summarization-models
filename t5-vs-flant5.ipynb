{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook we explore T5 vs. FlanT5 on the MuP dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tannaaman/investigating-summarization-models/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (/home/tannaaman/.cache/huggingface/datasets/allenai___csv/allenai--mup-c30ba3347ec8183d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paper_name': ['Practical Locally Private Federated Learning with Communication Efficiency'],\n",
       " 'text': [\"1 INTRODUCTION . 1.1 BACKGROUND . Federated learning ( FL ) Kairouz et al . ( 2019 ) ; Konečnỳ et al . ( 2016 ) is a rapidly evolving application of distributed optimization to large-scale learning or estimation scenarios where multiple entities . called clients , collaborate in solving a machine learning problem , under the coordination of a central server . Each client ’ s raw data is stored locally and not exchanged or transferred . To achieve the learning objective , the server collects minimal information from the clients for immediate aggregation . FL is particularly suitable for mobile and edge device applications since the ( sensitive ) individual data never directly leave the device and has seen deployments in industries ( ? Hard et al. , 2019 ; Leroy et al. , 2019 ) . While FL offers significant practical privacy improvements over centralizing all the training data , it lacks a formal privacy guarantee . As discussed in Melis et al . ( 2018 ) , even if only model updates ( i.e . gradient updates ) are transmitted , it is easy to compromise the privacy of individual clients . Differential privacy ( DP ) ( Dwork et al. , 2014 ) is the state-of-the-art approach to address information disclosure . Differentially private algorithms fuse participation of any individual via injecting algorithm-specific random noise . In FL setting , DP is suitable for protecting against external adversaries , i.e . a malicious analyst that tries to infer individual data via observing final or intermediate model results . However , DP paradigms typically assume a trusted curator , which corresponds to the server in the FL setting . This assumption is often not satisfied in practical cases , under which users that act as clients may not trust the service provider that acts as the server . Local differential privacy ( LDP ) ( Kasiviswanathan et al. , 2011 ; Dwork et al. , 2014 ) provides privacy protection on the individual level via applying randomized mechanisms that obfuscate the data before leaving the client , and is a more natural privacy model for distributed learning scenarios like FL , allowing easier compliance with regulatory strictures ( Bhowmick et al. , 2018 ) . 1.2 METHODOLOGY . We will be focusing on distributed learning procedures that aim to solve an empirical risk minimization problem in a decentralized fashion : min θ∈Θ L ( θ ) , L ( θ ) = 1 N M∑ m=1 Lm ( θ ) ( 1 ) where M denotes the number of clients . Let Yi denotes training data that belongs to the m-th client consisting Nm data points , and N = ∑M m=1Nm . The term Lm ( θ ) = ∑ y∈Ym ` ( θ , y ) stands for locally aggregated loss evaluated at parameter θ , where ` : Θ × Y 7→ R+ is the loss function depending on the problem context and we use Θ and Y to describe parameter and data domain respectively . We will seek to optimize ( 1 ) via gradient-based methods . In what follows we will use [ n ] , n ∈ N+ to denote the set { 1 , 2 , . . . , n } . At round t , the procedure iterates as : 1 . Server distribute the current value θt among a subset S of clients . 2 . For each client s ∈ S , a local update is computed ∆s = g ( θt , Ys ) and transmitted to the server . 3 . The server aggregates all { ∆s } s∈Ss to obtain a global update ∆ and updates the parameter as θt+1 = θt + ∆ . We distinguish 2 practical scenarios as cross-silo FL and cross-device FL ( Kairouz et al. , 2019 ) . In cross-silo FL , M is relatively small ( i.e . M ≤ 100 ) and usually each client has a moderate or large amount of data ( i.e . minmNm 1 ) . As a consequence , all the clients participate in the learning process ( i.e . S = [ M ] ) . In each iteration a client computes locally a negative stochastic gradient g ( θt , Ys ) = − 1R ∑ y∈Rs ∇ ` ( θt , y ) of Ls ( θt ) /Ns , based on a uniform random subsample Rs ⊂ Ys of size R. In cross-device FL S is a uniformly random subset of [ M ] , and g ( θt , Ys ) = − 1Ns ∑ y∈Ys ∇ ` ( θt , y ) is the average negative gradient over Ys evaluated at θt . In this paper we will be focusing on the FedSgd aggregation rule corresponding to a stochastic gradient step with learning rate η , i.e. , ∆ = η ∑ s∈S Ns N g ( θt , Ys ) . Generally speaking , there are two sources of data that need privacy protection : the parameter θt and the updates { ∆s } s∈S . In this paper we will develop algorithms for protecting { ∆s } s∈S against inference attack or reconstruction attack , which will be defined later . Note that the protection of θts could be done via applying standard central DP techniques as in Bhowmick et al . ( 2018 ) . To begin our discussion on suitably defined privacy models , we first review the local model of privacy . Local differential privacy Kasiviswanathan et al . ( 2011 ) we say a randomized algorithm A that maps the private data X ∈ X to some value Z = A ( X ) ∈ Z is -locally differentially private , if the induced conditional probability measure P ( ·|X = x ) satisfies that for any x , x′ ∈ X and any Z ∈ Z : e− ≤ P ( Z|X = x ) P ( Z|X = x′ ) ≤ e ( 2 ) In the FL setting , the private data X is typically not the raw data Y but the gradient of the loss function ` evaluated at Y . Hereafter we will assume the private data to be a d dimensional euclidean vector , while d could be very large ( i.e . to the order of millions ) . LDP is a very strong privacy protection model that protects individual data against inference attacks that aims at inferring the membership of arbitrary data from the data universe . According to the discussion in Bhowmick et al . ( 2018 ) , allowing adversaries with such strength may be overly pessimistic : to conduct an effective inference attack the adversary shall come up with `` the true data '' and only use the privatized output to verify his/her belief about the membership . If the prior information is reasonably constrained for the adversary , we may adopt a weaker , but still elegant type of privacy protection paradigm that protects against reconstruction attacks . Heuristically , reconstruction attack aims at recovering individual data with respect to a well-defined criterion , given a prior over the data domain that is not too concentrated . Formally , we adopt the definition in Bhowmick et al . ( 2018 ) : Reconstruction attack Bhowmick et al . ( 2018 ) Let π be a prior distribution over the data domain Y that encodes the adversary ’ s prior belief . We describe the generation process of privatized user data as Y → X → Z = A ( X ) for some ( privacy-protecting ) mechanism A. Additionally let f : Y 7→ Rk be the target of reconstruction ( i.e. , the adversary wants to evaluate the value f ( Y ) ) and Lrec : Rk × Rk 7→ R+ be the reconstruction loss serving as the criterion . Then an estimator ζ : X 7→ Rk provides an ( α , p , f ) -reconstruction breach for the loss Lrec if there exists some z ∈ Z such that : P ( Lrec ( f ( X ) , ζ ( z ) ) |A ( X ) = z ) > p ( 3 ) The existence of a reconstruction breach provides an attack that effectively breaks the privatized algorithm for some output z . Hence to provide protection against reconstruction attacks , we need the following to hold : sup ζ sup z∈Z P ( Lrec ( f ( X ) , ζ ( z ) ) |A ( X ) = z ) ≤ p ( 4 ) When ( 4 ) holds , the mechanism A is said to be ( α , p , f ) -protected against reconstruction for the loss Lrec . In ( Bhowmick et al. , 2018 , Lemma 2.2 ) , the authors relates LDP mechanisms with protection against reconstruction attacks , and the result for several scenarios suggest that using LDP mechanisms with large may still provide decent protection against reconstruction . As a consequence , we identify two sorts of threats used in this paper : Inference attacks by powerful adversaries this is the standard setup in LDP scenarios under which the adversary can observe the final learned model , as well as the information about potential participants except for the precise membership list . For such cases , only low LDP mechanisms provide sufficient protection against inference attacks . Reconstruction attacks by curious onlookers in this relaxed scenario we assume the adversary is able to observe all model updates and individual communications to conduct reconstruction attacks that target the evaluation of some function f over private individual data . But knowledge about individual data is restricted some prior distribution πf . For such cases , we will discover mechanisms that are based upon LDP algorithms with large but provides decent protection against reconstruction . Moreover , we assume the adversaries to be semi-honest ( Lyu et al. , 2020 ) , i.e . they are curious about individual data but follow the FL protocol correctly . In the seminal work ( Duchi et al. , 2018 ) , the authors developed minimax optimal LDP mean estimators under i.i.d generative constraints . Bhowmick et al . ( 2018 ) used separate mechanisms to privatize both the direction and the scale of the individual gradients , and was shown to achieve comparable performance with respect to non-private counterparts using high s. Despite the effectiveness of the LDP approach , the communication cost 1 is prohibitive . For models involving deep neural architectures , the number of parameters explodes to the order of millions . For each round , each client transmit O ( dδ ) bits to the server , where δ is the minimum number of bits to represent a real number with desired precision . The resulting communication cost is usually not affordable for mobile scenarios . In fact , the primary bottleneck for FL is usually communication , especially for deep neural architectures ( Kairouz et al. , 2019 ) . It is thus of significant importance to reduce the communication cost to a reasonable level . We identify two main challenges in building practical privacy-preserving FL algorithms : Communication Efficiency The algorithm shall be communication efficient , measured in terms of bits transferred per client in a single round . High-dimensional compatibility It was shown in Duchi et al . ( 2018 ) that upon estimating a ddimensional vector , the locally private constraint incurs a multiplicative penalty of O ( d/ 2 ) in terms of minimax ` 2 risk . This linear dependence on the dimension of gradients may result in overly noisy gradient estimates that destroy the learning process . Hence the algorithm shall be able to handle models with high-dimensional inputs with tolerable performance degradation . 1In general , there are two sources of communication cost in a distributed learning scenario that requires sequential interactions between the server and the clients : downlink cost that happens during clients downloading the updated model from the central server , and uplink cost that happens when clients sending their updates to the server . In was previously noted in Konečnỳ et al . ( 2016 ) that uplink cost usually dominates downlink costs in FL settings . Hence in the scope of this work , we will refer to communication cost as uplink communication cost . Summary of contributions In this paper , we propose a practical solution to locally private FL setting that addresses the above concerns which we term selective quantized stochastic gradient descent ( sqSGD ) . Our contributions are summarized as follows : 1 . We propose a novel algorithm for locally private multivariate mean estimation . The proposed algorithm stochastically quantizes each dimension of each sample to K equally spaced levels and serves as the base algorithm for our gradient-based learning framework . 2 . We make several improvements to the basic algorithm under the gradient-based learning scenario . Specifically , we attribute the estimation error of the base algorithm to perturbation error caused by privacy constraints , and quantization error . Then we utilize a gradient subsampling strategy to simultaneously reduce communication cost and improve private estimation utility . We also apply randomized rotation to reduce quantization error . 3 . We verify the performance of sqSGD on benchmark datasets using standard neural architectures like LeNet ( Lecun et al. , 1998 ) and ResNet ( He et al. , 2016 ) , and make systematic studies on the impact of both communication and privacy constraints . Specifically , under the same privacy and communication constraints , our model outperforms baseline algorithms that do not involve quantization by a significant margin .\"],\n",
       " 'summary': ['This paper studies FL under local differential privacy constraints. They identify two major concerns in designing practical privacy-preserving FL algorithms: communication efficiency and high\\x02dimensional compatibility, and develop a gradient-based learning algorithm sqSGD that addresses both concerns. They improve the base algorithm in two ways: First, apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs. Secondly, utilize randomized rotation as a preprocessing step to reduce quantization error. '],\n",
       " 'paper_id': ['SP:7f4b788b00a2a10bcd60351c3e04c8f597101e96']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load MuP dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"allenai/mup\" # allenai/mup-full\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "dataset[:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization prompts\n",
    "We take the prompts from the templates [here](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py: \n",
    "* XSum\n",
    "* Gigaword\n",
    "* CNN daily mail\n",
    "\n",
    "Note that we only choose those that do summarization, not expansion (given summary, write article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Summarize:\\n\\n{text}\\n\\nSummary:', '{summary}'),\n",
      " ('Summarize this article:\\n\\n{text}\\n\\nSummary:', '{summary}'),\n",
      " ('Summarize this article in one sentence.\\n\\n{text}\\n\\nSummary:', '{summary}'),\n",
      " ('{text}\\nWhat is a summary of this text?', '{summary}'),\n",
      " ('{text}\\nWhat was that article about?', '{summary}'),\n",
      " ('{text}\\n\\nThis article was about:', '{summary}'),\n",
      " ('Article:{text}\\n\\nA summary of the above article is?', '{summary}'),\n",
      " ('Article:{text}\\n\\nSummarize the main points of that article.', '{summary}'),\n",
      " ('Write a short summary for this text: {text}\\n\\nSummary:', '{summary}'),\n",
      " ('Briefly summarize this sentence: {text}\\n\\nSummary:', '{summary}'),\n",
      " ('Generate a short summary this sentence:\\n{text}\\n\\nSummary:', '{summary}'),\n",
      " ('What is a shorter version of this:\\n\\n{text}\\n\\nSummary:', '{summary}'),\n",
      " ('{text}\\n\\nWrite a brief summary in a sentence or less.', '{summary}'),\n",
      " ('{text}\\n\\nWhat is a very short summary of the above text?', '{summary}'),\n",
      " ('{text}\\nSummarize the aforementioned text in a single phrase.', '{summary}'),\n",
      " ('{text}\\nCan you generate a short summary of the above paragraph?',\n",
      "  '{summary}'),\n",
      " ('Write highlights for this article:\\n\\n{text}\\n\\nHighlights:',\n",
      "  '{highlights}'),\n",
      " ('Write some highlights for the following article:\\n\\n{text}\\n\\nHighlights:',\n",
      "  '{highlights}'),\n",
      " ('{text}\\n\\nWrite highlights for this article.', '{highlights}'),\n",
      " ('{text}\\n\\nWhat are highlight points for this article?', '{highlights}'),\n",
      " ('{text}\\nSummarize the highlights of this article.', '{highlights}'),\n",
      " ('{text}\\nWhat are the important parts of this article?', '{highlights}'),\n",
      " ('{text}\\nHere is a summary of the highlights for this article:',\n",
      "  '{highlights}')]\n"
     ]
    }
   ],
   "source": [
    "all_prompts = [\n",
    "    (\"Summarize:\\n\\n{text}\\n\\nSummary:\", \"{summary}\"),\n",
    "        (\"Summarize this article:\\n\\n{text}\\n\\nSummary:\", \"{summary}\"),\n",
    "        (\"Summarize this article in one sentence.\\n\\n{text}\\n\\nSummary:\",\n",
    "         \"{summary}\"),\n",
    "        (\"{text}\\nWhat is a summary of this text?\", \"{summary}\"),\n",
    "        (\"{text}\\nWhat was that article about?\", \"{summary}\"),\n",
    "        (\"{text}\\n\\nThis article was about:\", \"{summary}\"),\n",
    "        (\"Article:{text}\\n\\nA summary of the above article is?\", \"{summary}\"),\n",
    "        (\"Article:{text}\\n\\nSummarize the main points of that article.\",\n",
    "         \"{summary}\"),\n",
    "        (\"Write an article based on this summary:\\n\\n{summary}\\n\\nArticle:\",\n",
    "         \"{text}\"),\n",
    "        (\"Write an article based on this \\\"{summary}\\\"\\n\\nArticle:\", \"{text}\"),\n",
    "         (\"Write a short summary for this text: {text}\\n\\nSummary:\",\n",
    "         \"{summary}\"),\n",
    "        (\"Briefly summarize this sentence: {text}\\n\\nSummary:\", \"{summary}\"),\n",
    "        (\"Generate a short summary this sentence:\\n{text}\\n\\nSummary:\",\n",
    "         \"{summary}\"),\n",
    "        (\"What is a shorter version of this:\\n\\n{text}\\n\\nSummary:\",\n",
    "         \"{summary}\"),\n",
    "        (\"{text}\\n\\nWrite a brief summary in a sentence or less.\", \"{summary}\"),\n",
    "        (\"{text}\\n\\nWhat is a very short summary of the above text?\",\n",
    "         \"{summary}\"),\n",
    "        (\"{text}\\nSummarize the aforementioned text in a single phrase.\",\n",
    "         \"{summary}\"),\n",
    "        (\"{text}\\nCan you generate a short summary of the above paragraph?\",\n",
    "         \"{summary}\"),\n",
    "        (\"Write a text based on this summary: {summary}\\n\\nText:\", \"{text}\"),\n",
    "        (\"Write a text based on \\\"{summary}\\\"\\n\\nText:\", \"{text}\"),\n",
    "        (\"Write highlights for this article:\\n\\n{text}\\n\\nHighlights:\",\n",
    "         \"{highlights}\"),\n",
    "        (\"Write some highlights for the following \"\n",
    "         \"article:\\n\\n{text}\\n\\nHighlights:\", \"{highlights}\"),\n",
    "        (\"{text}\\n\\nWrite highlights for this article.\", \"{highlights}\"),\n",
    "        (\"{text}\\n\\nWhat are highlight points for this article?\",\n",
    "         \"{highlights}\"),\n",
    "        (\"{text}\\nSummarize the highlights of this article.\", \"{highlights}\"),\n",
    "        (\"{text}\\nWhat are the important parts of this article?\",\n",
    "         \"{highlights}\"),\n",
    "        (\"{text}\\nHere is a summary of the highlights for this article:\",\n",
    "         \"{highlights}\"),\n",
    "        (\"Write an article using the following \"\n",
    "         \"points:\\n\\n{highlights}\\n\\nArticle:\", \"{text}\"),\n",
    "        (\"Use the following highlights to write an \"\n",
    "         \"article:\\n\\n{highlights}\\n\\nArticle:\", \"{text}\"),\n",
    "        (\"{highlights}\\n\\nWrite an article based on these highlights.\",\n",
    "         \"{text}\"),\n",
    "        \n",
    "]\n",
    "\n",
    "summarization_prompts = [p for p in all_prompts if \"{text}\" in p[0].lower()]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(summarization_prompts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def generate_summaries(model: AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer, prompt: str, documents_to_summarize: List[str], max_length=150, num_beams=4, no_repeat_ngram_size=2, early_stopping=True):\n",
    "    \"\"\"\n",
    "    Takes in a prompt and list of documents to summarize and returns a list of summaries. Keyword for replacement in the prompt is {text}.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    batch_size = 4\n",
    "\n",
    "    # 0. model to device\n",
    "    model.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(0, len(documents_to_summarize), batch_size):\n",
    "            batch = documents_to_summarize[i:i+batch_size]\n",
    "\n",
    "            # 1. include prompt for every document\n",
    "            prompt_batch = [prompt.format(text=text) for text in batch]\n",
    "\n",
    "            # 2. tokenize prompt and move to device\n",
    "            inputs = tokenizer(prompt_batch, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "\n",
    "            # 3. generate summary\n",
    "            outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams, no_repeat_ngram_size=no_repeat_ngram_size, early_stopping=early_stopping).to(\"cpu\")\n",
    "            summaries.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/tannaaman/.cache/huggingface/datasets/allenai___csv/allenai--mup-c30ba3347ec8183d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e5c346cff315c88f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating for google/flan-t5-large with prompt 0 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 1 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 2 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 3 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 4 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 5 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 6 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 7 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 8 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 9 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 10 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 11 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 12 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 13 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 14 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 15 and output format {summary}...\n",
      "generating for google/flan-t5-large with prompt 16 and output format {highlights}...\n",
      "generating for google/flan-t5-large with prompt 17 and output format {highlights}...\n",
      "generating for google/flan-t5-large with prompt 18 and output format {highlights}...\n",
      "generating for google/flan-t5-large with prompt 19 and output format {highlights}...\n",
      "generating for google/flan-t5-large with prompt 20 and output format {highlights}...\n",
      "generating for google/flan-t5-large with prompt 21 and output format {highlights}...\n",
      "generating for google/flan-t5-large with prompt 22 and output format {highlights}...\n",
      "generating for t5-small with prompt 0 and output format {summary}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tannaaman/investigating-summarization-models/env/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating for t5-small with prompt 1 and output format {summary}...\n",
      "generating for t5-small with prompt 2 and output format {summary}...\n",
      "generating for t5-small with prompt 3 and output format {summary}...\n",
      "generating for t5-small with prompt 4 and output format {summary}...\n",
      "generating for t5-small with prompt 5 and output format {summary}...\n",
      "generating for t5-small with prompt 6 and output format {summary}...\n",
      "generating for t5-small with prompt 7 and output format {summary}...\n",
      "generating for t5-small with prompt 8 and output format {summary}...\n",
      "generating for t5-small with prompt 9 and output format {summary}...\n",
      "generating for t5-small with prompt 10 and output format {summary}...\n",
      "generating for t5-small with prompt 11 and output format {summary}...\n",
      "generating for t5-small with prompt 12 and output format {summary}...\n",
      "generating for t5-small with prompt 13 and output format {summary}...\n",
      "generating for t5-small with prompt 14 and output format {summary}...\n",
      "generating for t5-small with prompt 15 and output format {summary}...\n",
      "generating for t5-small with prompt 16 and output format {highlights}...\n",
      "generating for t5-small with prompt 17 and output format {highlights}...\n",
      "generating for t5-small with prompt 18 and output format {highlights}...\n",
      "generating for t5-small with prompt 19 and output format {highlights}...\n",
      "generating for t5-small with prompt 20 and output format {highlights}...\n",
      "generating for t5-small with prompt 21 and output format {highlights}...\n",
      "generating for t5-small with prompt 22 and output format {highlights}...\n",
      "generating for t5-base with prompt 0 and output format {summary}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tannaaman/investigating-summarization-models/env/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating for t5-base with prompt 1 and output format {summary}...\n",
      "generating for t5-base with prompt 2 and output format {summary}...\n",
      "generating for t5-base with prompt 3 and output format {summary}...\n",
      "generating for t5-base with prompt 4 and output format {summary}...\n",
      "generating for t5-base with prompt 5 and output format {summary}...\n",
      "generating for t5-base with prompt 6 and output format {summary}...\n",
      "generating for t5-base with prompt 7 and output format {summary}...\n",
      "generating for t5-base with prompt 8 and output format {summary}...\n",
      "generating for t5-base with prompt 9 and output format {summary}...\n",
      "generating for t5-base with prompt 10 and output format {summary}...\n",
      "generating for t5-base with prompt 11 and output format {summary}...\n",
      "generating for t5-base with prompt 12 and output format {summary}...\n",
      "generating for t5-base with prompt 13 and output format {summary}...\n",
      "generating for t5-base with prompt 14 and output format {summary}...\n",
      "generating for t5-base with prompt 15 and output format {summary}...\n",
      "generating for t5-base with prompt 16 and output format {highlights}...\n",
      "generating for t5-base with prompt 17 and output format {highlights}...\n",
      "generating for t5-base with prompt 18 and output format {highlights}...\n",
      "generating for t5-base with prompt 19 and output format {highlights}...\n",
      "generating for t5-base with prompt 20 and output format {highlights}...\n",
      "generating for t5-base with prompt 21 and output format {highlights}...\n",
      "generating for t5-base with prompt 22 and output format {highlights}...\n",
      "generating for t5-large with prompt 0 and output format {summary}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tannaaman/investigating-summarization-models/env/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating for t5-large with prompt 1 and output format {summary}...\n",
      "generating for t5-large with prompt 2 and output format {summary}...\n",
      "generating for t5-large with prompt 3 and output format {summary}...\n",
      "generating for t5-large with prompt 4 and output format {summary}...\n",
      "generating for t5-large with prompt 5 and output format {summary}...\n",
      "generating for t5-large with prompt 6 and output format {summary}...\n",
      "generating for t5-large with prompt 7 and output format {summary}...\n",
      "generating for t5-large with prompt 8 and output format {summary}...\n",
      "generating for t5-large with prompt 9 and output format {summary}...\n",
      "generating for t5-large with prompt 10 and output format {summary}...\n",
      "generating for t5-large with prompt 11 and output format {summary}...\n",
      "generating for t5-large with prompt 12 and output format {summary}...\n",
      "generating for t5-large with prompt 13 and output format {summary}...\n",
      "generating for t5-large with prompt 14 and output format {summary}...\n",
      "generating for t5-large with prompt 15 and output format {summary}...\n",
      "generating for t5-large with prompt 16 and output format {highlights}...\n",
      "generating for t5-large with prompt 17 and output format {highlights}...\n",
      "generating for t5-large with prompt 18 and output format {highlights}...\n",
      "generating for t5-large with prompt 19 and output format {highlights}...\n",
      "generating for t5-large with prompt 20 and output format {highlights}...\n",
      "generating for t5-large with prompt 21 and output format {highlights}...\n",
      "generating for t5-large with prompt 22 and output format {highlights}...\n",
      "generating for google/flan-t5-base with prompt 0 and output format {summary}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 217kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 37.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:01<00:00, 1.65MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 504kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 141kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 990M/990M [00:11<00:00, 89.5MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 11.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating for google/flan-t5-base with prompt 1 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 2 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 3 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 4 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 5 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 6 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 7 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 8 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 9 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 10 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 11 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 12 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 13 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 14 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 15 and output format {summary}...\n",
      "generating for google/flan-t5-base with prompt 16 and output format {highlights}...\n",
      "generating for google/flan-t5-base with prompt 17 and output format {highlights}...\n",
      "generating for google/flan-t5-base with prompt 18 and output format {highlights}...\n",
      "generating for google/flan-t5-base with prompt 19 and output format {highlights}...\n",
      "generating for google/flan-t5-base with prompt 20 and output format {highlights}...\n",
      "generating for google/flan-t5-base with prompt 21 and output format {highlights}...\n",
      "generating for google/flan-t5-base with prompt 22 and output format {highlights}...\n",
      "generating for google/flan-t5-small with prompt 0 and output format {summary}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 574kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 36.1MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 3.22MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 361kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 238kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 308M/308M [00:02<00:00, 110MB/s]  \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 25.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating for google/flan-t5-small with prompt 1 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 2 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 3 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 4 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 5 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 6 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 7 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 8 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 9 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 10 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 11 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 12 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 13 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 14 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 15 and output format {summary}...\n",
      "generating for google/flan-t5-small with prompt 16 and output format {highlights}...\n",
      "generating for google/flan-t5-small with prompt 17 and output format {highlights}...\n",
      "generating for google/flan-t5-small with prompt 18 and output format {highlights}...\n",
      "generating for google/flan-t5-small with prompt 19 and output format {highlights}...\n",
      "generating for google/flan-t5-small with prompt 20 and output format {highlights}...\n",
      "generating for google/flan-t5-small with prompt 21 and output format {highlights}...\n",
      "generating for google/flan-t5-small with prompt 22 and output format {highlights}...\n"
     ]
    }
   ],
   "source": [
    "from evaluation_utils import evaluate_rouge_score\n",
    "model_names = [\n",
    "    \"google/flan-t5-large\",\n",
    "    \"t5-small\",\n",
    "    \"t5-base\",\n",
    "    \"t5-large\",\n",
    "    \"google/flan-t5-base\",\n",
    "    \"google/flan-t5-small\", \n",
    "]\n",
    "sample_size = 16\n",
    "dataset = dataset.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "out = dict() # model -> prompt -> rouge scores\n",
    "for model_name in model_names: \n",
    "    if model_name not in out: \n",
    "        out[model_name] = dict()\n",
    "    prompt_count = 0\n",
    "    for prompt, output_format in summarization_prompts:\n",
    "        \n",
    "        print(f\"generating for {model_name} with prompt {prompt_count} and output format {output_format}...\")\n",
    "    \n",
    "        # 1. load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        # 2. generate summaries\n",
    "        summaries = generate_summaries(model, tokenizer, prompt, documents_to_summarize=dataset[\"text\"])\n",
    "\n",
    "        # 3. do evaluation\n",
    "        rouge_scores = evaluate_rouge_score(summaries, dataset[\"summary\"])\n",
    "\n",
    "        # 4. save results\n",
    "        out[model_name][prompt] = rouge_scores\n",
    "        prompt_count += 1\n",
    "\n",
    "# save to pickle\n",
    "import pickle\n",
    "with open(\"t5-vs-flant5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(out, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average by model\n",
      "{'google/flan-t5-base': tensor(0.1789),\n",
      " 'google/flan-t5-large': tensor(0.1709),\n",
      " 'google/flan-t5-small': tensor(0.1798),\n",
      " 't5-base': tensor(0.3292),\n",
      " 't5-large': tensor(0.4032),\n",
      " 't5-small': tensor(0.2919)}\n",
      "\n",
      "Average by prompt\n",
      "{'Article:{text}\\n\\nA summary of the above article is?': tensor(0.9923),\n",
      " 'Article:{text}\\n\\nSummarize the main points of that article.': tensor(0.9923),\n",
      " 'Briefly summarize this sentence: {text}\\n\\nSummary:': tensor(1.3868),\n",
      " 'Generate a short summary this sentence:\\n{text}\\n\\nSummary:': tensor(0.9381),\n",
      " 'Summarize this article in one sentence.\\n\\n{text}\\n\\nSummary:': tensor(1.3837),\n",
      " 'Summarize this article:\\n\\n{text}\\n\\nSummary:': tensor(1.2982),\n",
      " 'Summarize:\\n\\n{text}\\n\\nSummary:': tensor(1.4133),\n",
      " 'What is a shorter version of this:\\n\\n{text}\\n\\nSummary:': tensor(1.4437),\n",
      " 'Write a short summary for this text: {text}\\n\\nSummary:': tensor(1.1277),\n",
      " 'Write highlights for this article:\\n\\n{text}\\n\\nHighlights:': tensor(1.4129),\n",
      " 'Write some highlights for the following article:\\n\\n{text}\\n\\nHighlights:': tensor(1.3601),\n",
      " '{text}\\n\\nThis article was about:': tensor(1.2483),\n",
      " '{text}\\n\\nWhat are highlight points for this article?': tensor(1.2483),\n",
      " '{text}\\n\\nWhat is a very short summary of the above text?': tensor(1.2483),\n",
      " '{text}\\n\\nWrite a brief summary in a sentence or less.': tensor(1.2483),\n",
      " '{text}\\n\\nWrite highlights for this article.': tensor(1.2483),\n",
      " '{text}\\nCan you generate a short summary of the above paragraph?': tensor(1.2483),\n",
      " '{text}\\nHere is a summary of the highlights for this article:': tensor(1.2483),\n",
      " '{text}\\nSummarize the aforementioned text in a single phrase.': tensor(1.2483),\n",
      " '{text}\\nSummarize the highlights of this article.': tensor(1.2483),\n",
      " '{text}\\nWhat are the important parts of this article?': tensor(1.2483),\n",
      " '{text}\\nWhat is a summary of this text?': tensor(1.2483),\n",
      " '{text}\\nWhat was that article about?': tensor(1.2483)}\n"
     ]
    }
   ],
   "source": [
    "# average by model, across prompts\n",
    "average_by_model = dict()\n",
    "for model_name in model_names:\n",
    "    results = out[model_name]\n",
    "    average_by_model[model_name] = sum(v for  r in results.values() for k, v in r.items() if \"_fmeasure\" in k) / len(results.values())\n",
    "\n",
    "# average by prompt, across models\n",
    "average_by_prompt = dict()\n",
    "for prompt, filler in summarization_prompts:\n",
    "    results = [out[model_name][prompt] for model_name in model_names]\n",
    "    average_by_prompt[prompt] = sum(v for r in results for k, v in r.items()) / len(results)\n",
    "\n",
    "# results\n",
    "print(\"Average by model\")\n",
    "pprint(average_by_model)\n",
    "print(\"\\nAverage by prompt\")\n",
    "pprint(average_by_prompt)\n",
    "\n",
    "# save to pickle\n",
    "with open(\"t5-vs-flant5-average-by-model-large-scale.pkl\", \"wb\") as f:\n",
    "    pickle.dump(average_by_model, f)\n",
    "with open(\"t5-vs-flant5-average-by-prompt.pkl-scale\", \"wb\") as f:\n",
    "    pickle.dump(average_by_prompt, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"t5-vs-flant5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investigating-summarization-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
